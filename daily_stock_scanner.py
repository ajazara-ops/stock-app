import yfinance as yf
import pandas as pd
import json
import time
import ssl
import requests
import xml.etree.ElementTree as ET
import math
import os
import sys
import re
import argparse 
from collections import Counter
from datetime import datetime, timedelta

# SSL ì¸ì¦ì„œ ì˜¤ë¥˜ ë°©ì§€
ssl._create_default_https_context = ssl._create_unverified_context

# [ì‹ ê·œ] í´ë” ê²½ë¡œ ìƒìˆ˜ ì •ì˜
DAILY_DATA_DIR = 'daily_data'
WEEKLY_REPORT_DIR = 'weekly_reports'

# --- [ì•ˆì „ì¥ì¹˜] ---
def safe_float(val, default=0.0):
    try:
        f = float(val)
        if math.isnan(f) or math.isinf(f): return default
        return f
    except: return default

# --- [ì•Œë¦¼ ì „ì†¡ í•¨ìˆ˜] ---
def send_push_notification(title, message):
    # âœ… ì‚¬ìš©ìë‹˜ì˜ í‘¸ì‹œ í† í°
    user_push_tokens = ["ExponentPushToken[kip5csOC92Ymcc_AtKjqyl]"] 

    if not user_push_tokens:
        print(f"âš ï¸ [ì•Œë¦¼ ì‹œë®¬ë ˆì´ì…˜] ì „ì†¡í•  í† í° ì—†ìŒ.\nì œëª©: {title}\në‚´ìš©: {message}")
        return

    url = "https://exp.host/--/api/v2/push/send"
    headers = {
        "host": "exp.host",
        "accept": "application/json",
        "accept-encoding": "gzip, deflate",
        "content-type": "application/json"
    }

    print(f"ğŸ“¨ ì•Œë¦¼ ì „ì†¡ ì‹œë„: {title}")
    for token in user_push_tokens:
        payload = {
            "to": token,
            "title": title,
            "body": message,
            "sound": "default",
            "priority": "high"
        }
        try:
            requests.post(url, headers=headers, data=json.dumps(payload))
        except Exception as e:
            print(f"  âŒ ì „ì†¡ ì—ëŸ¬: {e}")

# --- [ì–´ì œ ì¶”ì²œ ì¢…ëª© ê°€ì ¸ì˜¤ê¸°] ---
def get_latest_recommendation_ids():
    # [ìˆ˜ì •] daily_data í´ë”ì—ì„œ ê²€ìƒ‰
    if not os.path.exists(DAILY_DATA_DIR): return set()
    files = sorted([f for f in os.listdir(DAILY_DATA_DIR) if f.endswith('_daily.json')], reverse=True)
    if not files: return set()
    try:
        with open(f"{DAILY_DATA_DIR}/{files[0]}", 'r', encoding='utf-8') as f:
            data = json.load(f)
            return {s['id'] for s in data.get('stocks', [])}
    except:
        return set()

# --- [ì–´ì œ ì¶”ì²œ ì¢…ëª© ê°€ì ¸ì˜¤ê¸° (ë‚ ì§œ ê¸°ì¤€, ë°±í•„ìš©)] ---
def get_previous_recommendation_ids(target_date_str):
    # [ìˆ˜ì •] daily_data í´ë”ì—ì„œ ê²€ìƒ‰
    if not os.path.exists(DAILY_DATA_DIR): return set()
    
    target_date = datetime.strptime(target_date_str, "%Y-%m-%d")
    files = sorted([f for f in os.listdir(DAILY_DATA_DIR) if f.endswith('_daily.json')], reverse=True)
    
    for f in files:
        file_date_str = f.split('_')[0]
        try:
            file_date = datetime.strptime(file_date_str, "%Y-%m-%d")
            if file_date < target_date:
                with open(f"{DAILY_DATA_DIR}/{f}", 'r', encoding='utf-8') as file:
                    data = json.load(file)
                    return {s['id'] for s in data.get('stocks', [])}
        except:
            continue
    return set()

# --- [1] ì‹œì¥ ìƒí™© ë¶„ì„ ---
def analyze_market_condition(target_date=None):
    print("ğŸŒ ê¸€ë¡œë²Œ ì‹œì¥ ìƒí™© ë¶„ì„ ì¤‘...")
    markets = {'US': {'ticker': '^GSPC', 'name': 'S&P 500'}, 'KR': {'ticker': '^KS11', 'name': 'KOSPI'}, 'VIX': {'ticker': '^VIX', 'name': 'ê³µí¬ì§€ìˆ˜'}}
    market_status = {}
    for key, info in markets.items():
        try:
            ticker = yf.Ticker(info['ticker'])
            hist = ticker.history(period="2y") # ë„‰ë„‰íˆ ê°€ì ¸ì˜´
            
            # ê³¼ê±° ì‹œì  ë¶„ì„
            if target_date:
                target_dt = datetime.strptime(target_date, "%Y-%m-%d")
                hist.index = hist.index.tz_localize(None) 
                hist = hist[hist.index <= target_dt]

            if len(hist) < 2:
                market_status[key] = {'status': 'UNKNOWN', 'change': 0.0, 'current': 0.0, 'message': 'ë°ì´í„° ì—†ìŒ'}
                continue
                
            current = hist['Close'].iloc[-1]; prev = hist['Close'].iloc[-2]
            change_pct = ((current - prev) / prev) * 100
            status, message = "NEUTRAL", ""
            
            if key == 'VIX':
                if current >= 30: status, message = "PANIC", "ê·¹ë„ì˜ ê³µí¬ ğŸ˜±"
                elif current >= 20: status, message = "BAD", "ê³µí¬ êµ¬ê°„ ğŸ˜¨"
                elif current <= 15: status, message = "VERY_GOOD", "ì‹œì¥ ê³¼ì—´ ğŸ¤‘"
                else: status, message = "NEUTRAL", "ì•ˆì •ì  ğŸ˜Œ"
            else:
                if change_pct >= 1.0: status, message = "VERY_GOOD", "ê°•í•œ ìƒìŠ¹ ğŸ”¥"
                elif change_pct >= 0.2: status, message = "GOOD", "ìƒìŠ¹ì„¸ ğŸ“ˆ"
                elif change_pct > -0.5: status, message = "NEUTRAL", "ë³´í•©ì„¸ â–"
                elif change_pct > -1.5: status, message = "BAD", "í•˜ë½ì„¸ â˜ï¸"
                else: status, message = "PANIC", "í­ë½ ê²½ê³  â›ˆï¸"
                
            market_status[key] = {'name': info['name'], 'current': safe_float(round(current, 2)), 'change': safe_float(round(change_pct, 2)), 'status': status, 'message': message}
        except Exception as e: 
            market_status[key] = {'status': 'UNKNOWN', 'change': 0.0, 'message': 'ë¶„ì„ ì‹¤íŒ¨'}
    return market_status

# --- [2] ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ---
def get_sp500_tickers():
    try:
        url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers)
        table = pd.read_html(response.text)
        tickers = table[0]['Symbol'].tolist()
        return [t.replace('.', '-') for t in tickers]
    except: return ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'META']

def get_nasdaq100_tickers():
    try:
        url = 'https://en.wikipedia.org/wiki/Nasdaq-100'
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers)
        tables = pd.read_html(response.text)
        for table in tables:
            if 'Ticker' in table.columns:
                return [str(t).replace('.', '-') for t in table['Ticker'].tolist()]
            elif 'Symbol' in table.columns:
                return [str(t).replace('.', '-') for t in table['Symbol'].tolist()]
        return []
    except Exception as e:
        print(f"âš ï¸ ë‚˜ìŠ¤ë‹¥ 100 ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []

def get_korea_tickers():
    tickers = []
    try:
        url = 'https://finance.naver.com/sise/sise_market_sum.naver?sosok=0&page=1'
        res = requests.get(url)
        codes = re.findall(r'href="/item/main.naver\?code=(\d{6})"', res.text)
        seen = set()
        unique_codes = [x for x in codes if not (x in seen or seen.add(x))]
        for code in unique_codes[:50]: tickers.append(f"{code}.KS")
    except Exception as e: print(f"âš ï¸ ì½”ìŠ¤í”¼ ëª©ë¡ ì‹¤íŒ¨: {e}")

    try:
        url = 'https://finance.naver.com/sise/sise_market_sum.naver?sosok=1&page=1'
        res = requests.get(url)
        codes = re.findall(r'href="/item/main.naver\?code=(\d{6})"', res.text)
        seen = set()
        unique_codes = [x for x in codes if not (x in seen or seen.add(x))]
        for code in unique_codes[:50]: tickers.append(f"{code}.KQ")
    except Exception as e: print(f"âš ï¸ ì½”ìŠ¤ë‹¥ ëª©ë¡ ì‹¤íŒ¨: {e}")

    if not tickers:
        return ['005930.KS', '000660.KS', '373220.KS', '005380.KS', '000270.KS', '068270.KS', '005490.KS', '035420.KS']
    return tickers

# --- [3] ë‰´ìŠ¤ ìˆ˜ì§‘ ---
def get_news_from_google_kr(ticker):
    try:
        url = f"https://news.google.com/rss/search?q={ticker.split('.')[0]}+ì£¼ê°€&hl=ko&gl=KR&ceid=KR:ko"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=3); root = ET.fromstring(response.content)
        return [{'title': item.find('title').text, 'link': item.find('link').text, 'publisher': item.find('source').text} for item in root.findall('./channel/item')[:3]]
    except: return []

def get_news_from_google_us(ticker):
    try:
        url = f"https://news.google.com/rss/search?q={ticker}+stock&hl=en-US&gl=US&ceid=US:en"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=3); root = ET.fromstring(response.content)
        return [{'title': item.find('title').text, 'link': item.find('link').text, 'publisher': item.find('source').text} for item in root.findall('./channel/item')[:3]]
    except: return []

def analyze_news_sentiment(news_list):
    analyzed = []
    pos = ['surge', 'jump', 'soar', 'gain', 'profit', 'buy', 'growth', 'record', 'upgrade', 'ê¸‰ë“±', 'ìƒìŠ¹', 'í˜¸ì¬', 'ì¦ê°€', 'ê°œì„ ', 'ë§¤ìˆ˜', 'ì²´ê²°', 'ì„±ì¥', 'ìµœê³ ']
    neg = ['drop', 'fall', 'plunge', 'loss', 'miss', 'sell', 'crash', 'downgrade', 'lawsuit', 'ê¸‰ë½', 'í•˜ë½', 'ì•…ì¬', 'ê°ì†Œ', 'ë¶€ì§„', 'ë§¤ë„', 'ì ì', 'ìš°ë ¤', 'ì†Œì†¡']
    for n in news_list:
        t = n.get('title', '').strip(); 
        if not t: continue
        t_l = t.lower(); sent = 'neutral'
        p_sc = sum(1 for w in pos if w in t_l); n_sc = sum(1 for w in neg if w in t_l)
        if p_sc > n_sc: sent = 'positive'
        elif n_sc > p_sc: sent = 'negative'
        analyzed.append({'title': t, 'link': n.get('link',''), 'sentiment': sent, 'publisher': n.get('publisher','News')})
    return analyzed

def process_news_for_list(stock_list):
    if not stock_list: return
    print(f"\nğŸ“° Top {len(stock_list)} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    for item in stock_list:
        ticker = item['id']; mkt = item['market']
        print(f"   [{mkt}] {ticker}...", end=' '); raw = []
        try:
            if mkt == 'KR': raw = get_news_from_google_kr(ticker)
            else: raw = get_news_from_google_us(ticker)
        except: pass
        print(f"{len(raw)}ê°œ"); item['news'] = analyze_news_sentiment(raw[:3])

# --- [4] ì§€í‘œ ê³„ì‚° ---
def calculate_indicators(close, high, low):
    delta = close.diff(); gain = (delta.where(delta > 0, 0)).rolling(14).mean(); loss = (-delta.where(delta < 0, 0)).rolling(14).mean(); rs = gain/loss; rsi = 100 - (100/(1+rs))
    exp1 = close.ewm(span=12).mean(); exp2 = close.ewm(span=26).mean(); macd = exp1 - exp2; signal = macd.ewm(span=9).mean()
    ma20 = close.rolling(20).mean(); std = close.rolling(20).std(); upper = ma20 + (std*2); lower = ma20 - (std*2)
    lowest_low = low.rolling(window=14).min()
    highest_high = high.rolling(window=14).max()
    stoch_k = 100 * ((close - lowest_low) / (highest_high - lowest_low))
    stoch_d = stoch_k.rolling(window=3).mean()
    return rsi, macd, signal, upper, lower, ma20, stoch_k, stoch_d

# --- [5] ê°œë³„ ì¢…ëª© ë¶„ì„ ---
def analyze_stock(ticker, market_type, target_date=None):
    try:
        stock = yf.Ticker(ticker)
        try: hist = stock.history(period="2y")
        except: return None
        
        # [ìˆ˜ì •] ê³¼ê±° ì‹œì  ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ìŠ¬ë¼ì´ì‹±
        if target_date:
            target_dt = datetime.strptime(target_date, "%Y-%m-%d")
            hist.index = hist.index.tz_localize(None)
            hist = hist[hist.index <= target_dt]

        if len(hist) < 120: return None
        
        info = {}
        try: info = stock.info 
        except: pass
        
        if market_type == 'US' and info.get('operatingMargins', 0) < -0.5: return None
        
        close = hist['Close']; volume = hist['Volume']; high = hist['High']; low = hist['Low']
        rsi, macd, signal, bb_upper, bb_lower, ma20, stoch_k, stoch_d = calculate_indicators(close, high, low)
        
        cur_p = close.iloc[-1]; cur_rsi = rsi.iloc[-1]; cur_low = bb_lower.iloc[-1]
        ma60 = close.rolling(60).mean().iloc[-1]
        ma120 = close.rolling(120).mean().iloc[-1]
        cur_k = stoch_k.iloc[-1]
        vol_ma20 = volume.rolling(20).mean().iloc[-1]; cur_vol = volume.iloc[-1]
        rvol = safe_float(cur_vol / vol_ma20, 1.0) if vol_ma20 > 0 else 1.0
        
        sector = info.get('sector', 'ê¸°íƒ€')
        if market_type == 'KR' and sector == 'ê¸°íƒ€':
            if ticker in ['005930.KS', '000660.KS']: sector = 'Technology'
            elif ticker in ['005380.KS', '000270.KS']: sector = 'Automotive'
            
        if pd.isna(cur_rsi) or pd.isna(cur_p) or cur_rsi > 80: return None
        
        score = 0; reasons = [] 
        
        # 1. RSI
        if cur_rsi < 30: score += 40; reasons.append("RSI ê³¼ë§¤ë„(ê°•ë ¥)")
        elif cur_rsi < 45: score += 20; reasons.append("ë‹¨ê¸° ê³¼ë§¤ë„")
        elif cur_rsi < 60: score += 5; reasons.append("ëˆŒë¦¼ëª© êµ¬ê°„")
        
        # 2. ë³¼ë¦°ì € ë°´ë“œ
        if cur_p <= cur_low * 1.05: score += 30; reasons.append("ë³¼ë¦°ì €ë°´ë“œ í•˜ë‹¨ ê·¼ì ‘")
        
        # 3. ì´í‰ì„  ì§€ì§€
        if not pd.isna(ma60) and cur_p >= ma60 * 0.98 and cur_p <= ma60 * 1.05: score += 20; reasons.append("60ì¼ì„  ì§€ì§€")
        
        # 4. MACD
        if macd.iloc[-1] > signal.iloc[-1]: score += 15; reasons.append("MACD ìƒìŠ¹ì‹ í˜¸")
        
        # 5. ê±°ë˜ëŸ‰
        if rvol >= 1.5: score += 20; reasons.append(f"ê±°ë˜ëŸ‰ í­ë°œ({rvol:.1f}ë°°)")
        elif rvol >= 1.2: score += 10; reasons.append(f"ê±°ë˜ëŸ‰ ì¦ê°€")
        
        # 6. ìŠ¤í† ìºìŠ¤í‹±
        if cur_k < 20: score += 15; reasons.append("ìŠ¤í† ìºìŠ¤í‹± ê³¼ë§¤ë„")
        
        # 7. ì¥ê¸° ì¶”ì„¸
        if not pd.isna(ma120) and cur_p >= ma120: score += 10; reasons.append("ì¥ê¸° ìƒìŠ¹ ì¶”ì„¸")

        # 8. í€ë”ë©˜í„¸
        op_margin = info.get('operatingMargins', 0)
        rev_growth = info.get('revenueGrowth', 0)
        per = info.get('forwardPE', info.get('trailingPE', 0))
        pbr = info.get('priceToBook', 0)

        if market_type == 'US':
            if op_margin > 0.15: score += 10; reasons.append("ì´ìµë¥  ìš°ìˆ˜")
            if rev_growth > 0.10: score += 10; reasons.append("ê³ ì„±ì¥ì£¼")
            if per > 0 and per < 30: score += 10; reasons.append("ì €í‰ê°€(PER)")
        elif market_type == 'KR':
            if per > 0 and per < 20: score += 5; reasons.append("ì ì • PER")
            if pbr > 0 and pbr < 1.5: score += 5; reasons.append("ì €PBR")
            if op_margin > 0: score += 5; reasons.append("í‘ì ê¸°ì—…")
        
        cutoff = 40 
        if score < cutoff: return None
        
        name = info.get('shortName', ticker) if info else ticker
        price_val = safe_float(round(cur_p, 2))
        
        hist_data = []
        for d, r in hist.iloc[-20:].iterrows():
            p = round(float(r['Close']), 2) if not math.isnan(r['Close']) else None
            b_u = round(float(bb_upper.loc[d]), 2) if not math.isnan(bb_upper.loc[d]) else None
            b_l = round(float(bb_lower.loc[d]), 2) if not math.isnan(bb_lower.loc[d]) else None
            hist_data.append({"time": d.strftime("%m-%d"), "price": p, "bb_upper": b_u, "bb_lower": b_l})

        return {
            "id": ticker, "rank": 0, "symbol": ticker.replace('.KS','').replace('.KQ',''), "name": name, "market": market_type,
            "currentPrice": price_val,
            "changePercent": safe_float(round(((cur_p - hist['Close'].iloc[-2]) / hist['Close'].iloc[-2]) * 100, 2)),
            "buyZoneTop": safe_float(round(cur_p * 1.02, 2), price_val), "buyZoneBottom": safe_float(round(cur_p * 0.98, 2), price_val),
            "targetPrice": safe_float(round(cur_p * 1.1, 2), price_val), "aiReason": " + ".join(reasons),
            "score": int(score), "rsi": safe_float(round(cur_rsi, 2)), "history": hist_data, "news": [],
            "financials": {"op_margin": safe_float(info.get('operatingMargins', 0)), "rev_growth": safe_float(info.get('revenueGrowth', 0)), "per": safe_float(info.get('forwardPE', 0))},
            "sector": sector, "rvol": safe_float(round(rvol, 2))
        }
    except: return None

# --- [6] ì£¼ê°„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± (ìˆ˜ìµë¥  ê²°ì‚°) ---
def generate_weekly_report(today_str):
    print(f"\nğŸ“Š [Weekly] ì§€ë‚œ 2ì£¼ê°„({today_str} ê¸°ì¤€)ì˜ í†µí•© ì„±ê³¼ ë¶„ì„ ì‹œì‘...")
    
    # [ìˆ˜ì •] daily_data í´ë”ì—ì„œ ìµœê·¼ 14ì¼ê°„ íŒŒì¼ ì½ê¸°
    daily_files = []
    end_date = datetime.strptime(today_str, "%Y-%m-%d")
    start_date = end_date - timedelta(days=14)
    
    if not os.path.exists(DAILY_DATA_DIR): 
        os.makedirs(DAILY_DATA_DIR)

    if not os.path.exists(WEEKLY_REPORT_DIR):
        os.makedirs(WEEKLY_REPORT_DIR)

    for f in os.listdir(DAILY_DATA_DIR):
        if f.endswith('_daily.json'):
            file_date_str = f.split('_')[0]
            try:
                file_date = datetime.strptime(file_date_str, "%Y-%m-%d")
                if start_date <= file_date <= end_date: 
                    daily_files.append(f)
            except: pass
            
    print(f"ğŸ“‚ ë¶„ì„ ëŒ€ìƒ ë°ì¼ë¦¬ íŒŒì¼: {len(daily_files)}ê°œ")
    
    aggregated_stocks = []
    for file in daily_files:
        with open(f"{DAILY_DATA_DIR}/{file}", 'r', encoding='utf-8') as f:
            data = json.load(f)
            rec_date = file.split('_')[0]
            for stock in data.get('stocks', []):
                stock['buyPrice'] = stock['currentPrice'] 
                stock['recommendDate'] = rec_date
                aggregated_stocks.append(stock)

    print(f"ğŸ” ì´ {len(aggregated_stocks)}ê°œì˜ ê³¼ê±° ì¶”ì²œ ë‚´ì—­ ìˆ˜ìµë¥  ê³„ì‚° ì¤‘...")

    final_results = []
    for i, item in enumerate(aggregated_stocks):
        ticker = item['id']
        buy_price = item['buyPrice']
        # print(f"[{i+1}/{len(aggregated_stocks)}] ìˆ˜ìµë¥  ê³„ì‚°: {ticker}...", end='\r')
        try:
            stock_info = yf.Ticker(ticker)
            todays_data = stock_info.history(period="5d")
            if len(todays_data) > 0:
                current_price = float(todays_data['Close'].iloc[-1])
                return_rate = ((current_price - buy_price) / buy_price) * 100
                item['currentPrice'] = round(current_price, 2)
                item['returnRate'] = round(return_rate, 2)
                final_results.append(item)
        except Exception as e: pass 

    # ìˆ˜ìµë¥  ìˆœ ì •ë ¬ (Top 10 ë“±ì€ ì•±ì—ì„œ ì²˜ë¦¬í•˜ê±°ë‚˜ ì—¬ê¸°ì„œ ë¯¸ë¦¬ ì˜ë¼ë„ ë¨. ì—¬ê¸°ì„  ì „ì²´ ì €ì¥)
    final_results.sort(key=lambda x: x['returnRate'], reverse=True)
    top_performers = final_results[:10]
    for i, item in enumerate(top_performers): item['rank'] = i + 1
        
    ms = analyze_market_condition()
    out = {
        "market_status": ms, 
        "stocks": top_performers, # ë¦¬í¬íŠ¸ì—ëŠ” Top 10ë§Œ ì €ì¥ (ìš©ëŸ‰ ì ˆì•½ ë° ì•± ë¡œì§ ì¼ì¹˜)
        "dominant_sectors": [], 
        "timestamp": f"{today_str} 08:00:00 (Weekly Report)"
    }
    
    # [ìˆ˜ì •] weekly_reports í´ë”ì— ì €ì¥
    output_path = f"{WEEKLY_REPORT_DIR}/{today_str}_weekly.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(out, f, indent=2, ensure_ascii=False, allow_nan=False)
        
    print(f"\nâœ… ì£¼ê°„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_path}")

# --- [7] ì£¼ê°„ ìˆ˜ìµë¥  ê²°ì‚° ì•Œë¦¼ (í† ìš”ì¼ 5PM) ---
def send_weekly_summary_notification():
    print(f"\nğŸ“¢ [Weekly Summary] ì£¼ê°„ ìˆ˜ìµë¥  ê²°ì‚° ì•Œë¦¼ ì „ì†¡ ì‹œì‘...")
    
    today_str = time.strftime("%Y-%m-%d")
    
    # [ìˆ˜ì •] weekly_reports í´ë”ì—ì„œ ì˜¤ëŠ˜ì ë¦¬í¬íŠ¸ í™•ì¸
    report_file_path = f"{WEEKLY_REPORT_DIR}/{today_str}_weekly.json"
    
    if not os.path.exists(report_file_path):
        print(f"âš ï¸ ì˜¤ëŠ˜ì({today_str}) ë¦¬í¬íŠ¸ê°€ ì—†ì–´ì„œ ìë™ ìƒì„±í•©ë‹ˆë‹¤...")
        generate_weekly_report(today_str)
        update_history_index()

    # ë¦¬í¬íŠ¸ íŒŒì¼ ì½ê¸°
    try:
        with open(report_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            stocks = data.get('stocks', [])
    except:
        print("âŒ ë¦¬í¬íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨")
        return

    # Top 10 í‰ê·  ê³„ì‚° (ì´ë¯¸ íŒŒì¼ì—ëŠ” ìˆ˜ìµë¥  ê³„ì‚°ëœ Top 10ì´ ë“¤ì–´ìˆìŒ)
    us_stocks = [s for s in stocks if s['market'] == 'US']
    kr_stocks = [s for s in stocks if s['market'] == 'KR']

    def calc_avg(stock_list):
        if not stock_list: return 0.0
        rets = [s['returnRate'] for s in stock_list]
        return sum(rets) / len(rets)

    us_avg = calc_avg(us_stocks)
    kr_avg = calc_avg(kr_stocks)
    
    print(f"ğŸ‡ºğŸ‡¸ ë¯¸êµ­ Top10 í‰ê· : {us_avg:.2f}%")
    print(f"ğŸ‡°ğŸ‡· í•œêµ­ Top10 í‰ê· : {kr_avg:.2f}%")

    title = "ğŸ“Š ì£¼ê°„ ìˆ˜ìµë¥  ê²°ì‚° ë„ì°©"
    body = "ì§€ë‚œ 2ì£¼ê°„ì˜ ì¶”ì²œ ì¢…ëª© ì„±ê³¼ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\nì§€ê¸ˆ ì•±ì—ì„œ í•œêµ­/ë¯¸êµ­ Top 10 ìˆ˜ìµë¥ ì„ í™•ì¸í•´ë³´ì„¸ìš”!"
    
    send_push_notification(title, body)

# --- [ìˆ˜ì •] ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (Weekly í´ë” ìŠ¤ìº”) ---
def update_history_index():
    if not os.path.exists(WEEKLY_REPORT_DIR): return
    hl = []
    for f in sorted(os.listdir(WEEKLY_REPORT_DIR), reverse=True):
        if f.endswith('_weekly.json'): 
            d_str = f.split('_')[0]
            # ì•±ì´ ì½ì„ ìˆ˜ ìˆëŠ” ê²½ë¡œë¡œ ì €ì¥ (weekly_reports í´ë” í¬í•¨)
            hl.append({"date": d_str, "file": f"{WEEKLY_REPORT_DIR}/{f}"})
                
    with open('history_index.json', 'w', encoding='utf-8') as f: json.dump(hl, f, indent=2, ensure_ascii=False)

# --- [ë°±í•„ ì‹¤í–‰ í•¨ìˆ˜] ---
def run_backfill(start_date, end_date):
    print(f"\nâª Backfill Mode: {start_date} ~ {end_date}")
    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    
    if not os.path.exists(DAILY_DATA_DIR): os.makedirs(DAILY_DATA_DIR)

    print("ğŸ“‹ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì¤‘...")
    sp500 = get_sp500_tickers()
    nasdaq100 = get_nasdaq100_tickers()
    us_tickers = list(set(sp500 + nasdaq100))
    kr_tickers = get_korea_tickers()
    
    current_dt = start_dt
    while current_dt <= end_dt:
        if current_dt.weekday() >= 5: # ì£¼ë§ ê±´ë„ˆë›°ê¸°
            current_dt += timedelta(days=1)
            continue
            
        target_str = current_dt.strftime("%Y-%m-%d")
        print(f"\nğŸ“… [Backfill] ì²˜ë¦¬ ì¤‘: {target_str}")
        
        ms = analyze_market_condition(target_date=target_str)
        final_stocks = []
        
        print(f"ğŸ‡ºğŸ‡¸ US Analyzing ({target_str})...")
        usc = []
        for i, t in enumerate(us_tickers):
            d = analyze_stock(t, 'US', target_date=target_str)
            if d: usc.append(d)
        usc.sort(key=lambda x: x['score'], reverse=True)
        ust = usc[:10]
        for i, item in enumerate(ust): item['rank'] = i + 1
        final_stocks.extend(ust)
        
        print(f"ğŸ‡°ğŸ‡· KR Analyzing ({target_str})...")
        krc = []
        for i, t in enumerate(kr_tickers):
            d = analyze_stock(t, 'KR', target_date=target_str)
            if d: krc.append(d)
        krc.sort(key=lambda x: x['score'], reverse=True)
        krt = krc[:10]
        for i, item in enumerate(krt): item['rank'] = i + 1
        final_stocks.extend(krt)
        
        # ë°±í•„ì—ì„œëŠ” ì•Œë¦¼ ì•ˆ ë³´ëƒ„ (ë‚´ìš©ë§Œ ìƒì„±)
        prev_stock_ids = get_previous_recommendation_ids(target_str)
        new_stocks = [s['symbol'] for s in final_stocks if s['id'] not in prev_stock_ids]
        
        noti_title = "ğŸ”” DailyPick10 ì•Œë¦¼"
        noti_body = ""
        if new_stocks:
            highlight_stocks = ", ".join(new_stocks[:2])
            noti_body = f"ì˜¤ëŠ˜ì˜ ì¶”ì²œ: {highlight_stocks} ë“± (ì‹ ê·œ {len(new_stocks)}ê±´)"
        else:
            top_stocks = ", ".join([s['symbol'] for s in final_stocks[:2]])
            noti_body = f"ì˜¤ëŠ˜ì˜ ì¶”ì²œ: {top_stocks} ë“± (ìˆœìœ„ ë³€ë™)"

        all_sectors = [s['sector'] for s in final_stocks if s['sector'] != 'ê¸°íƒ€']
        dominant_sectors = [item[0] for item in Counter(all_sectors).most_common(2)]

        out = {
            "market_status": ms, 
            "stocks": final_stocks, 
            "dominant_sectors": dominant_sectors, 
            "timestamp": f"{target_str} 16:00:00", 
            "notification": { "title": noti_title, "body": noti_body }
        }
        
        # [ìˆ˜ì •] daily_data í´ë”ì— ì €ì¥
        filename = f"{DAILY_DATA_DIR}/{target_str}_daily.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(out, f, indent=2, ensure_ascii=False, allow_nan=False)
            
        print(f"ğŸ’¾ Saved: {filename}")
        current_dt += timedelta(days=1)

    # ë°±í•„ ëë‚œ í›„ ì¸ë±ìŠ¤ëŠ” ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ (ë°ì¼ë¦¬ íŒŒì¼ì€ ì¸ë±ìŠ¤ì— ì•ˆ ë„£ìŒ)
    print("\nâœ… Backfill Complete!")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', type=str, default='daily', help='Execution mode: daily, weekly, weekly_summary, or backfill')
    parser.add_argument('--target', type=str, default='ALL', help='Target market: US, KR, or ALL')
    parser.add_argument('--start', type=str, default=None, help='Backfill start date (YYYY-MM-DD)')
    parser.add_argument('--end', type=str, default=None, help='Backfill end date (YYYY-MM-DD)')
    args = parser.parse_args()
    
    today_str = time.strftime("%Y-%m-%d")
    print(f"ğŸš€ AI ì£¼ì‹ ë¶„ì„ê¸° ê°€ë™ (ëª¨ë“œ: {args.mode}, íƒ€ê²Ÿ: {args.target}, ë‚ ì§œ: {today_str})")

    if args.mode == 'daily':
        if not os.path.exists(DAILY_DATA_DIR):
            os.makedirs(DAILY_DATA_DIR)

        prev_stock_ids = get_latest_recommendation_ids()
        existing_stocks = []
        try:
            with open('todays_recommendation.json', 'r', encoding='utf-8') as f:
                existing_stocks = json.load(f).get('stocks', [])
        except: pass

        ms = analyze_market_condition()
        final_stocks = []
        
        if args.target in ['US', 'ALL']:
            sp500 = get_sp500_tickers()
            nasdaq100 = get_nasdaq100_tickers()
            us_tickers = list(set(sp500 + nasdaq100))
            print(f"\nğŸ‡ºğŸ‡¸ ë¯¸êµ­ ë¶„ì„ (ëŒ€ìƒ: {len(us_tickers)}ê°œ)...")
            usc = []
            for i, t in enumerate(us_tickers): 
                print(f"[{i+1}/{len(us_tickers)}] {t}...", end='\r'); d = analyze_stock(t, 'US'); 
                if d: usc.append(d)
            usc.sort(key=lambda x: x['score'], reverse=True); ust = usc[:10]
            for i, item in enumerate(ust): item['rank'] = i + 1
            process_news_for_list(ust)
            final_stocks.extend(ust)
        else:
            print("\nğŸ‡ºğŸ‡¸ ë¯¸êµ­ ë°ì´í„°ëŠ” ê¸°ì¡´ ë‚´ìš©ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
            us_kept = [s for s in existing_stocks if s['market'] == 'US']
            final_stocks.extend(us_kept)

        if args.target in ['KR', 'ALL']:
            kr = get_korea_tickers(); krc = []
            print(f"\nğŸ‡°ğŸ‡· í•œêµ­ ë¶„ì„ (ëŒ€ìƒ: {len(kr)}ê°œ)...")
            for i, t in enumerate(kr): 
                print(f"[{i+1}/{len(kr)}] {t}...", end='\r'); d = analyze_stock(t, 'KR'); 
                if d: krc.append(d)
            krc.sort(key=lambda x: x['score'], reverse=True); krt = krc[:10]
            for i, item in enumerate(krt): item['rank'] = i + 1
            process_news_for_list(krt)
            final_stocks.extend(krt)
        else:
            print("\nğŸ‡°ğŸ‡· í•œêµ­ ë°ì´í„°ëŠ” ê¸°ì¡´ ë‚´ìš©ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
            kr_kept = [s for s in existing_stocks if s['market'] == 'KR']
            final_stocks.extend(kr_kept)
        
        all_sectors = [s['sector'] for s in final_stocks if s['sector'] != 'ê¸°íƒ€']
        dominant_sectors = [item[0] for item in Counter(all_sectors).most_common(2)]
        
        noti_title = "ğŸ”” DailyPick10 ì•Œë¦¼"
        noti_body = ""
        
        target_market_stocks = [s for s in final_stocks if s['market'] == args.target] if args.target != 'ALL' else final_stocks
        
        if not target_market_stocks:
            print("ğŸ”• ì¶”ì²œ ì¢…ëª©ì´ ì—†ì–´ì„œ ì•Œë¦¼ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            new_stocks = [s['symbol'] for s in target_market_stocks if s['id'] not in prev_stock_ids]
            market_name = "ë¯¸êµ­" if args.target == 'US' else ("í•œêµ­" if args.target == 'KR' else "ì „ì²´")
            
            if new_stocks:
                highlight_stocks = ", ".join(new_stocks[:2])
                noti_body = f"ì˜¤ëŠ˜ì˜ {market_name} ì¶”ì²œ ì¢…ëª©ì´ ë„ì°©í–ˆìŠµë‹ˆë‹¤! ì‹ ê·œì§„ì…: {highlight_stocks} ë“± {len(target_market_stocks)}ê±´"
            else:
                top_stocks = ", ".join([s['symbol'] for s in target_market_stocks[:2]])
                noti_body = f"ì˜¤ëŠ˜ì˜ {market_name} ì¶”ì²œ ì¢…ëª©ì´ ë„ì°©í–ˆìŠµë‹ˆë‹¤! ì˜¤ëŠ˜ì˜ ì¶”ì²œ: {top_stocks} ë“± {len(target_market_stocks)}ê±´"

        out = {
            "market_status": ms, 
            "stocks": final_stocks, 
            "dominant_sectors": dominant_sectors, 
            "timestamp": f"{today_str} {datetime.now().strftime('%H:%M:%S')}",
            "notification": {
                "title": noti_title,
                "body": noti_body
            }
        }
        
        print("\nğŸ’¾ [Daily Mode] ì˜¤ëŠ˜ì˜ ì¶”ì²œ ì¢…ëª© ê°±ì‹  ì¤‘...")
        with open('todays_recommendation.json', 'w', encoding='utf-8') as f: json.dump(out, f, indent=2, ensure_ascii=False, allow_nan=False)

        # [ìˆ˜ì •] Daily ëª¨ë“œì¼ ë•Œ daily_data í´ë”ì— ë°±ì—…
        print(f"\nğŸ’¾ [Daily Backup] ë°ì¼ë¦¬ ë°ì´í„° ì €ì¥ ì¤‘... ({today_str})")
        with open(f"{DAILY_DATA_DIR}/{today_str}_daily.json", 'w', encoding='utf-8') as f:
            json.dump(out, f, indent=2, ensure_ascii=False, allow_nan=False)
        
        # [ì¤‘ìš”] ë°ì¼ë¦¬ ì‹¤í–‰ ì‹œì—ëŠ” ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì•ˆ í•¨ (ë¦¬í¬íŠ¸ ëª©ë¡ì— ë°ì¼ë¦¬ ì•ˆ ëœ¨ê²Œ)
        # update_history_index() <- ì œê±°

        if noti_body:
            send_push_notification(noti_title, noti_body)
    
    elif args.mode == 'weekly':
        generate_weekly_report(today_str)
        update_history_index()
    
    elif args.mode == 'weekly_summary':
        send_weekly_summary_notification()
    
    elif args.mode == 'backfill':
        if args.start and args.end:
            run_backfill(args.start, args.end)
        else:
            print("âš ï¸ Backfill mode requires --start and --end arguments (YYYY-MM-DD)")

    print(f"\nâœ… ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__": main()